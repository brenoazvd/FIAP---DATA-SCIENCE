# -*- coding: utf-8 -*-
"""Deep Learning e AI_ANOTAções.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iheN_DLwPs35C6Iyw44y4PmFvCA-JfXb

## Anotações Primeiro Semestre - Deep Learning e AI
## Breno Rodrigues Azevedo

# Pré-processamento de dados

*   Dropar colunas irrelevantes para o modelo
*   Dropar colunas com muitos NaNs
*   Dropar linhas com NaNs
*   Aplicar One-Hot Encoding em dados categóricos
*   Normalizar ou Padronizar dados Númericos Contínuos
*   Separar o seu dataset em Teste e Treino
1.  Padronizar = (Distribuição Normal)  
2.  Normalizar = (Quando está tudo cagado)

Normalização

N = X - Xmin / Xmax - Xmin
"""

def normalizar(x):
  return (x - np.min(x))/(np.max(x) - np.min(x))

"""Desvio padrão

(Não é necessário decorar a formula)
"""



"""Padronização

P = X - Xmed / Desvio padrão
"""

def padronizar(x):
    return (x -np.mean(x))/np.std(x)

"""# One Hot Encoding = Técnica de pré processamento para dados categóricos."""

df = pd.get_dummies(df, columns=[''])

"""## Metodo Gráfico

(curva galciana)
"""

import seaborn as sb
sb.pairplot(df[['idade',  'parentes', 'dependentes' ,'tarifa', 'sobreviveu']], hue='sobreviveu')

"""# Teste de Shapiro"""

from scipy.stats import shapiro
# Verificar normalidade usando o teste de Shapiro-Wilk
def verifica_normalidade(dataframe, coluna):
    coluna_data = dataframe[coluna]
    # Realizar o teste de Shapiro-Wilk
    statistic, p_valor = shapiro(coluna_data)
    # Definir o nível de significância
    nivel_significancia = 0.1
    # Verificar se a hipótese nula de normalidade pode ser rejeitada
    if p_valor > nivel_significancia:
        print(f"A coluna '{coluna}' segue uma distribuição normal")
        return True
    else:
        print(f"A coluna '{coluna}' não segue uma distribuição normal")
        return False

df.columns.to_list() #Para verificar as colunas. PS: Coluna classe não se coloca no texte de shapiro.

"""O código abaixo vai padronizar ou normalizar os dados das colunas selecionadas do dataset"""

# Chamando a função para verificar normalidade
for coluna in ['idade', 'parentes', 'dependentes',  'tarifa'  ]:
  if verifica_normalidade(df, coluna):
    df[coluna] = padronizar(df[coluna])#se for distribuição normal, padroniza
  else:#senão, normaliza
    df[coluna] = normalizar(df[coluna])                       #df - dataset que eu criei no começo do código, como não normalizamos e nem padronizamos a coluna classe...
                                                              # Podemos utilizar o código a seguir para selecionar apenas as entradas
                                                              # entradas = df.drop('class', axis=1)
                                                              # entradas.head(1)
                                                              # e este para a coluna "class"
                                                              # saida = df['class']
                                                              # saida.head(1)

"""#Teste e treino (é comum)

Comum: 70% amostras para treino -> no máximo 80% treino

Comum 30% amostras para teste -> no mín 20% test

A gente sempre usa as entradas de dados como X máiusculo
 (indica que são mais de uma entrada) entradas: todas as outras colunas com exceção 'sobreviveu'

 Exemplo teste (trains.csv)
"""

from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

y = df['sobreviveu']#classe (saída)
X = df.drop('sobreviveu', axis=1)#entradas

# Dividir o conjunto de dados em treino e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

#A gente sempre usa as entradas de dados como X máiusculo
# (idica que são mais de uma entra) entradas: todas as outras colunas com exceção 'sobreviveu'

"""**Regra** para achar o "k" ideal, olhar a raíz quadrada das linhas "[0]" do dataset e verificar se é ímpar."""

X.shape[0]**0.5 #acuracia do modelo

knn = KNeighborsClassifier(n_neighbors=27)

#treinar o modelo kNN
knn.fit(X_train, y_train)

"""## Acuracia"""

y_pred = knn.predict(X_test)

#Avaliar a acurácia do modelo
accuracy = accuracy_score(y_test, y_pred)
print('Acurácia do modelo kNN:', accuracy)

#testando o k ideal
for k in range(3,51,2):
  knn = KNeighborsClassifier(n_neighbors=k)
  # Treinar o modelo kNN
  knn.fit(X_train, y_train)
  y_pred = knn.predict(X_test)

  # Avaliar a acurácia do modelo
  accuracy = accuracy_score(y_test, y_pred)
  print(f'Acurácia para k={k}:', accuracy)

"""# Distancia Euclidiana (KNN)"""

#                                   0  1  2
def distancia_euclidiana(A, B): # A=[3, 6, 9]
#                                B=[7, 8, 12]
  if len(A) == len(B):
    total = 0
    for i in range(0, len(A)):
      total +- (B[i]- A[i])**2
    return total**0.5
  print("erro de dimensões")
  return 0

pontoA = [3, 6, 9]
pontoB = [7, 8, 12]
distancia_euclidiana(pontoA, pontoB)

"""###Kmeans

"""

#import matplotlib.pyplot as plt
#from sklearn.cluster import KMeans

df = np.array(entradas)
kmeans = KMeans(n_clusters=#?, random_state=0)
kmeans.fit(df) #().fit) Treina o modelo PS: O dataset precisa estar pré-processado | Processo de clusterização

kmeans.predict([x]) #ele preve um novo dado para classificar | por exemplo: 5.0,3.6,1.4,0.2 (dimensões da flor)
                    #kmeans.predict([[5.0,3.6,1.4,0.2]]) Iris-setosa

kmeans.labels_ #Verificar todas as linhas os seus predicts

#https://www.naftaliharris.com/blog/visualizing-k-means-clustering/
#site simulador de Kmeans: Processo de clusterização (visualização e informações)

exemplo = np.array(entradas.iloc[[0]])
previsao = kmeans.predict(exemplo)
print(f"Previsão {previsao} Valor real: {df['class'].iloc[0]}")

"""## Método Cotovelo"""

#Crianção da função
def soma_quadrados_intra_clusters(dataset):                 # K = clusters  0   1   2   3... 19
  wcss= [] #Aqui conseguimos todos os clusters e os resultados. ex: xcss = [92, 42, 27, 8...,1]
  for n in range(2, 21):
    kmeans = KMeans(n_clusters=n)
    kmeans.fit(dataset)
    wcss.append(kmeans.inertia_)
  return wcss

n_clusters = list(range(2,21))
somas = soma_quadrados_intra_clusters(dados)
plt.plot(n_clusters, somas, '-o', color='red')
plt.plot([2,20], [somas[0], somas[-1]])
plt.xlabel("Número de clusters")
plt.ylabel("Soma dos quadrados intra-clusters")

def numero_otimo_clusters(wcss):
  x0=1
  y0=wcss[0]
  x1=20
  y1=wcss[-1]

  distancias = []

  for i in range(len(wcss)):
    x = i + 2
    y = wcss[i]

    numerador = abs((y1-y0)*x -  (x1-x0)*y + x1*y0 - y1*x0)
    denominador = ((y1-y0)**2 + (x1-x0)**2)**0.5
    distancias.append(numerador/denominador)#.append = acrescentar | Serve para acrescentar o resultado
                                             #| do código ou da situação para a lista que você criou acima.
  print(distancias)
  return distancias.index(max(distancias))+2# O +2 serve para adicionar ao cluster que começa com 0 na contagem do wcss
                                             #(python em si) e o cluster começa com 2 (pois não é possível ter apenas 1, nãofazsentido)

k = numero_otimo_clusters(somas)
k

"""## DEEP LEARNING"""

import numpy as np

inputs =  np.array([ 1,   2,    3])
weights = np.array([0.1, 0.2, 0.3])
bias = 1.25

def somatorio (entradas, pesos):
  '''total = 0
  for i in range(0,len(inputs)):
    total += entradas[i]*pesos[i]
  return'''
  return entradas.dot(pesos)

def degrau(soma):
  if soma >= 1:
    return 1
  return 0

def sigmoide(soma,bias):
  return 1 / (1 + np.exp(-(u + bias)))

u = somatorio(inputs, weights)
print(u)

output = degrau(u)
print(output)

#Exercicio DOIS

inputs =  np.array([ -1,   5,    0.2])
weights = np.array([0.11, -0.2, 4])

def somatorio (entradas, pesos):
  '''total = 0
  for i in range(0,len(inputs)):
    total += entradas[i]*pesos[i]
  return'''
  return entradas.dot(pesos)

def degrau(soma):
  if soma >= 1:
    return 1
  return 0

def sigmoide(soma,bias):
  return 1 / (1 + np.exp(-bias*soma))

u = somatorio(inputs, weights)
print(u)

output = sigmoide(u, bias)
print(output)

"""Biblioteca de multi camadas"""

from sklearn.neural_network import MLPClassifier

"""rede neural"""

#Montando a arquitetura da rede
#objeto                    orientação objeto
redeneural = MLPClassifier(verbose=True, #Para printar a "epóca"(deixa lerdo) false=datasets reais
                           max_iter=10000, #máximo de interações
                           tol=0.000001,#tolerância
                           activation='relu', #Função de ativação
                           #hidden_layer_sizes=(30,30,30, #camadas ocultas
                           learning_rate_init=0.0001 #taxa de aprendizado
                           )

redeneural.fit(entradas,saidas)

"""##CODS

Gera todas as informações do data set
"""

!pip install skimpy
from skimpy import skim
skim(#dataset#)

"""Verificar todas as colunas"""

dataset#.columns.to_list()

"""abs = módulo | O módulo de um número sempre será positivo, pois ele representa uma distância variável positiva."""

abs

""".append = acrescentar | Serve para acrescentar o resultado do código ou da situação para a lista que você criou acima."""

[lista].append

"""Biblioteca para datasets (Toy Datasets)"""

from sklearn import datasets
#Exemplo
# iris = datasets.load_iris()

