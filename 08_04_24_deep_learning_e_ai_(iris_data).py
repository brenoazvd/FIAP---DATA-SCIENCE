# -*- coding: utf-8 -*-
"""08_04_24 Deep Learning e AI (Iris.data).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zBUA4HGPAKEF8zNvIIbgqDIdNknD5taX

## Análise e Classificação de Dados Iris com K-Means e Clusterização

# Breno Azevedo
"""

#Importando as dependências
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sb
from sklearn.cluster import KMeans
#"from" ele chama um código especifico de uma biblioteca maior (para deixar mais otimizado)

def normalizar(x):
  return (x - np.min(x))/(np.max(x) - np.min(x))

def padronizar(x):
    return (x -np.mean(x))/np.std(x)

df = pd.read_csv("/home/iris.data", names=['sepal_length', 'sepal_width', 'petal_length', 'petal_width','class'])
df

import seaborn as sb
sb.pairplot(df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']], hue='class')

from scipy.stats import shapiro
# Verificar normalidade usando o teste de Shapiro-Wilk
def verifica_normalidade(dataframe, coluna):
    coluna_data = dataframe[coluna]
    # Realizar o teste de Shapiro-Wilk
    statistic, p_valor = shapiro(coluna_data)
    # Definir o nível de significância
    nivel_significancia = 0.1
    # Verificar se a hipótese nula de normalidade pode ser rejeitada
    if p_valor > nivel_significancia:
        print(f"A coluna '{coluna}' segue uma distribuição normal")
        return True
    else:
        print(f"A coluna '{coluna}' não segue uma distribuição normal")
        return False

entradas = df.drop('class', axis=1)
entradas.head(1)

entradas.columns.to_list()

saida = df['class']
saida.head(1)

# Chamando a função para verificar normalidade
for coluna in ['sepal_length', 'sepal_width', 'petal_length', 'petal_width'  ]:
  if verifica_normalidade(df, coluna):
    df[coluna] = padronizar(df[coluna])#se for distribuição normal, padroniza
  else:#senão, normaliza
    df[coluna] = normalizar(df[coluna])

df

print("Coluna     |Máx   |   Min")
for colum in entradas.columns.to_list(): #imprimindo o valor maximo e minimo de cada coluna
  print(colum,'|', entradas[colum].max(),'|', entradas[colum].min())
  print('____________________________')

#sepal_length    sepal_width     petal_length     petal_width     class
#[ 5.1,          3.5,            1.4,             0.2,            Iris-setosa]
compr_sepal = (5.1 - 4.3) / (7.9 - 4.3)
larg_sepal = (3.5 - 2.0) / (4.4 - 2.0)
compr_petal = (1.4 - 1.0) / (6.9 - 1.0)
larg_petal = (0.2 - 0.1) / (2.5 - 0.1)
x = [compr_petal, larg_sepal, compr_petal, larg_petal]
x

#!pip install skimpy
#from skimpy import skim
#skim(entradas)

df['class']

df['class'] = df['class'].replace({'Iris-setosa': 0,
                                   'Iris-versicolor': 1,
                                   'Iris-virginica': 2})

#import matplotlib.pyplot as plt
#from sklearn.cluster import KMeans

dados = np.array(entradas) #crie uma váriavel diferente do "df" para não sobreescrever, como no exemplo que foi adicionado dados.
kmeans = KMeans(n_clusters=3, random_state=0)
kmeans.fit(dados) #(.fit) Treina o modelo PS: O dataset precisa estar pré-processado | Processo de clusterização

kmeans.predict([x]) #ele preve um novo dado para classificar | por exemplo: 5.0,3.6,1.4,0.2 (dimensões da flor)
                    #kmeans.predict([[5.0,3.6,1.4,0.2]]) Iris-setosa

kmeans.predict([[5.0,3.6,1.4,0.2]])#Iris-setosa

kmeans.predict([[7.0,3.2,4.7,1.4]])#Iris-versicolor

kmeans.predict([[4.9,2.5,4.5,1.7]])#Iris-virginica

kmeans.labels_

exemplo = np.array(entradas.iloc[[0]])
previsao = kmeans.predict(exemplo)
print(f"Previsão {previsao} Valor real: {df['class'].iloc[0]}")

def soma_quadrados_intra_clusters(dataset):                 # K = clusters  0   1   2   3... 19
  wcss= [] #Aqui conseguimos todos os clusters e os resultados. ex: xcss = [92, 42, 27, 8...,1]
  for n in range(2, 21):
    kmeans = KMeans(n_clusters=n)
    kmeans.fit(dataset)
    wcss.append(kmeans.inertia_)
  return wcss

n_clusters = list(range(2,21))
somas = soma_quadrados_intra_clusters(dados)
plt.plot(n_clusters, somas, '-o', color='red')
plt.plot([2,20], [somas[0], somas[-1]])
plt.xlabel("Número de clusters")
plt.ylabel("Soma dos quadrados intra-clusters")

def numero_otimo_clusters(wcss):
  x0=1
  y0=wcss[0]
  x1=20
  y1=wcss[-1]

  distancias = []

  for i in range(len(wcss)):
    x = i + 2
    y = wcss[i]

    numerador = abs((y1-y0)*x -  (x1-x0)*y + x1*y0 - y1*x0)
    denominador = ((y1-y0)**2 + (x1-x0)**2)**0.5
    distancias.append(numerador/denominador)#.append = acrescentar | Serve para acrescentar o resultado
                                             #| do código ou da situação para a lista que você criou acima.
  print(distancias)
  return distancias.index(max(distancias))+2# O +2 serve para adicionar ao cluster que começa com 0 na contagem do wcss
                                             #(python em si) e o cluster começa com 2 (pois não é possível ter apenas 1, nãofazsentido)

k = numero_otimo_clusters(somas)
k

